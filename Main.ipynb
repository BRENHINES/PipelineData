{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# <u>This is our Mini project for the course \"Fundamental Data Concepts\" (4DACF) at SUPINFO Lyon :</u>\n",
    "\n",
    "## <u>Evaluation Project - Data Processing and Visualization :</u>\n",
    "\n",
    "## <u>Project Objective :</u>\n",
    "You will design a complete data processing pipeline that includes several key steps: anonymization, transformation, cleaning, and data visualization.\n",
    "\n",
    "\n",
    "The goal is to leverage multiple technologies to produce a high-quality pipeline that adheres to best practices.\n",
    "\n",
    "This project must be carried out in groups of up to three students.\n",
    "\n",
    "## <u>Contexte :</u>\n",
    "A fictional e-commerce company aims to leverage its customer and transaction data while complying with GDPR regulations.\n",
    "\n",
    "The company has a dataset containing sensitive information and seeks to obtain:\n",
    "\n",
    "- [ ] An automated pipeline for anonymizing, transforming, and cleaning the data in python.\n",
    "- [ ] A final output optimized for direct use in Power BI."
   ],
   "id": "8a77807f92f26ee"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# <u>Step 1:</u> Pipeline Preparation: Python code for anonymization, cleaning, and transformation",
   "id": "497a74daa0aa671d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Importing the necessary libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ],
   "id": "324f03d0f7486edc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1. Exploratory Data Analysis (EDA)\n",
    "\n",
    "The first step in any data processing pipeline is to understand the data. This involves exploring the data to identify patterns, trends, and potential issues. EDA is a critical step that helps data engineer understand the data and make informed decisions about how to process it."
   ],
   "id": "7261471387cd8c79"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "2c6a95f4da68885d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2. Data Cleaning\n",
    "\n",
    "Data cleaning is the process of identifying and correcting errors in the data. This step is essential for ensuring the quality of the data and the accuracy of the analysis. Data cleaning involves several key tasks, including:\n",
    "- [ ] Handling missing values\n",
    "- [ ] Removing duplicates\n",
    "- [ ] Correcting errors\n",
    "- [ ] Standardizing data\n",
    "- [ ] Handling outliers\n",
    "- [ ] Encoding categorical variables\n",
    "- [ ] Feature engineering\n",
    "- [ ] Handling skewed data\n",
    "- [ ] Handling time series data\n"
   ],
   "id": "2e2f061bfb760744"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "52d4bbd49a9c8d51"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3. Data Transformation (Anonymization, pseudonomization, columns selection)\n",
    "Data transformation is the process of converting raw data into a format that is suitable for analysis. This step involves several key tasks, including:\n",
    "- [ ] Anonymization\n",
    "- [ ] Pseudonymization\n",
    "- [ ] Aggregation\n",
    "- [ ] Encoding\n",
    "- [ ] Data discretization\n",
    "- [ ] Data imputation\n",
    "- [ ] Data integration\n",
    "- [ ] Data reduction\n",
    "- [ ] Data wrangling\n",
    "- [ ] Data munging\n",
    "- [ ] Data fusion\n",
    "- [ ] Data harmonization"
   ],
   "id": "ce79e579e4a2500d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "f45d8161d8edf1a7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 4. Validation\n",
    "Data validation is the process of ensuring that the data is accurate, complete, and consistent. This step involves several key tasks, including:\n",
    "- [ ] Data profiling\n",
    "- [ ] Data quality assessment\n",
    "- [ ] Data integrity checks\n",
    "- [ ] Data validation rules\n",
    "- [ ] Data validation checks\n",
    "- [ ] Data validation methods\n",
    "- [ ] Data validation techniques\n",
    "- [ ] Data validation tools"
   ],
   "id": "cd16d3b57713f8e6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "fbc121e30a2defb3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 5. Data Export (To CSV or Excel)\n",
    "The final step in the data processing pipeline is to export the cleaned and transformed data to a file format that can be used for analysis. This step involves exporting the data to a CSV or Excel file, which can then be imported into a data visualization tool for further analysis."
   ],
   "id": "6f1e70ade75cca53"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "eb6212e08230c2dc"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
