{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# <u>This is our Mini project for the course \"Fundamental Data Concepts\" (4DACF) at SUPINFO Lyon :</u>\n",
    "\n",
    "## <u>Evaluation Project - Data Processing and Visualization :</u>\n",
    "\n",
    "## <u>Project Objective :</u>\n",
    "You will design a complete data processing pipeline that includes several key steps: anonymization, transformation, cleaning, and data visualization.\n",
    "\n",
    "\n",
    "The goal is to leverage multiple technologies to produce a high-quality pipeline that adheres to best practices.\n",
    "\n",
    "This project must be carried out in groups of up to three students.\n",
    "\n",
    "## <u>Contexte :</u>\n",
    "A fictional e-commerce company aims to leverage its customer and transaction data while complying with GDPR regulations.\n",
    "\n",
    "The company has a dataset containing sensitive information and seeks to obtain:\n",
    "\n",
    "- [ ] An automated pipeline for anonymizing, transforming, and cleaning the data in python.\n",
    "- [ ] A final output optimized for direct use in Power BI.\n",
    "\n",
    "## <u>BONUS :</u>\n",
    "\n",
    "- [ ] A set of visualizations in Python that provide insights into the data."
   ],
   "id": "8a77807f92f26ee"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# <u>Step 1:</u> Pipeline Preparation: Python code for anonymization, cleaning, and transformation",
   "id": "497a74daa0aa671d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from calendar import month\n",
    "\n",
    "# Importing the necessary libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import hashlib"
   ],
   "id": "324f03d0f7486edc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1. Exploratory Data Analysis (EDA)\n",
    "\n",
    "The first step in any data processing pipeline is to understand the data. This involves exploring the data to identify patterns, trends, and potential issues. EDA is a critical step that helps data engineer understand the data and make informed decisions about how to process it."
   ],
   "id": "7261471387cd8c79"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load the dataset : Mini_Projet_Evaluation.csv\n",
    "\n",
    "dataset_file_path: str = 'data/raw/Mini_Projet_Evaluation.csv'\n",
    "dataset = pd.read_csv(dataset_file_path)"
   ],
   "id": "2c6a95f4da68885d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Explore the data : Structure of the data, data types, etc.\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "\n",
    "print(f\"The dataset head is : \\n {dataset.head()}\") #to see a quick view of the dataset"
   ],
   "id": "f385c12f1c057b3f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Display the infos of the dataset\n",
    "\n",
    "print(f\"The dataset info is : \\n {dataset.info()}\") #to see the structure of the dataset"
   ],
   "id": "7b92923c35a5f7c7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Display the shape of the dataset\n",
    "\n",
    "print(f\"This dataset have {dataset.shape[0]} entries and {dataset.shape[1]} columns.\") #to see the number of rows and columns in resume, and to see the eventual problems on the dataset"
   ],
   "id": "24103c662aa52a6e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Display the descriptive statistics of the dataset\n",
    "\n",
    "print(f\"The dataset describe is : \\n {dataset.describe()}\") #to see the statistical summary of the dataset, to see the eventual problems on the dataset (outliers, etc.)"
   ],
   "id": "63ac21133446afed",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Display the data types in the dataset\n",
    "\n",
    "print(f\"The dataset data types are : \\n {dataset.dtypes}\") #to see the data types of the dataset"
   ],
   "id": "f2ae2e4ffb539d28",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2. Data Cleaning\n",
    "\n",
    "Data cleaning is the process of identifying and correcting errors in the data. This step is essential for ensuring the quality of the data and the accuracy of the analysis. Data cleaning involves several key tasks, including:\n",
    "- [X] Handling missing values\n",
    "- [X] Removing duplicates\n",
    "- [X] Correcting errors\n",
    "- [X] Correcting incoherent values\n",
    "- [X] Standardizing data\n",
    "- [X] Handling outliers\n",
    "- [X] Standardizing data types\n"
   ],
   "id": "2e2f061bfb760744"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Data Cleaning : Cleaning the dataset\n",
    "\n",
    "# Handling missing values\n",
    "\n",
    "missing_values_count = dataset.isnull().sum()\n",
    "print(f\"The missing values count is : \\n {missing_values_count}\") #to see the number of missing values in each column of the dataset"
   ],
   "id": "52d4bbd49a9c8d51",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Removing duplicates\n",
    "\n",
    "duplicates_values_count = dataset.duplicated().sum()\n",
    "print(f\"The number of duplicates in the dataset is : {duplicates_values_count}\") #to see the number of duplicates in the dataset"
   ],
   "id": "67896144ccc3232e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Correcting errors\n",
    "\n",
    "columns_to_verify = ['Sexe', 'Cat√©gorieProduitPr√©f√©r√©', 'AvisClient', 'AbonnementNewsletter', 'TypePaiementFavori', 'StatutCompte'] #to see the columns that we want to verify\n",
    "\n",
    "for col in columns_to_verify:\n",
    "    print(f\"\\nüîπ {col} : {dataset[col].nunique()} unique values\") #to see the number of unique values in each column of the dataset\n",
    "    print(dataset[col].unique())  # to see the unique values in each column of the dataset"
   ],
   "id": "cfcaac8ddeb35178",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Correcting Incoherent values\n",
    "\n",
    "# Incoherence detection\n",
    "incoherence_achats = dataset[(dataset['NombreAchats'] == 0) & (dataset['MontantTotalAchats'] > 0)]\n",
    "incoherence_panier = dataset[(dataset['MontantTotalAchats'] == 0) & (dataset['PanierMoyen'] > 0)]\n",
    "remboursements_invalides = dataset[dataset['MontantTotalRembours√©'] > dataset['MontantTotalAchats']]\n",
    "\n",
    "print(f\"üìå Nombre de pages incoh√©rentes (NombreAchats = 0 & MontantTotalAchats > 0) : {len(incoherence_achats)}\")\n",
    "print(f\"üìå Nombre de pages incoh√©rentes (MontantTotalAchats = 0 & PanierMoyen > 0) : {len(incoherence_panier)}\")\n",
    "print(f\"üìå Nombre d'enregistrements o√π le MontantTotalRembours√© > MontantTotalAchats :, {len(remboursements_invalides)}\")\n",
    "\n",
    "# Correcting with mean function\n",
    "dataset.loc[(dataset['NombreAchats'] == 0) & (dataset['MontantTotalAchats'] > 0), 'NombreAchats'] = dataset['NombreAchats'].median()\n",
    "dataset.loc[(dataset['Fr√©quenceAchatMensuel'] == 0) & (dataset['PanierMoyen'] > 0), 'Fr√©quenceAchatMensuel'] = dataset['Fr√©quenceAchatMensuel'].median()\n",
    "dataset.loc[(dataset['MontantTotalAchats'] == 0) & (dataset['PanierMoyen'] > 0), 'PanierMoyen'] = 0  # Logic : if the total amount of purchases is 0, the average basket is 0\n",
    "\n",
    "# deletion of the incoherent records\n",
    "df = dataset[dataset['MontantTotalRembours√©'] <= dataset['MontantTotalAchats']]"
   ],
   "id": "1e494ebaad6f6c8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Standardizing data\n",
    "\n",
    "# Standardizing the float columns\n",
    "columns_to_round = ['MontantTotalAchats', 'SoldeCompte', 'PanierMoyen', 'MontantTotalRembours√©'] #to see the columns that we want to round\n",
    "for col in columns_to_round:\n",
    "    dataset[col] = dataset[col].apply(lambda x: round(x, 2)) #to round the values in each column of the dataset\n",
    "\n",
    "# Standardizing the countries and cities columns\n",
    "standardize_name_columns = ['Pays', 'Ville'] #to see the columns that we want to standardize\n",
    "for col in standardize_name_columns:\n",
    "    dataset[col] = dataset[col].str.title() #to standardize the values in each column of the dataset\n",
    "\n",
    "# Standardizing the date columns\n",
    "standardize_date_columns = ['DateNaissance', 'DernierAchat', 'DateExpirationCarte'] #to see the columns that we want to standardize\n",
    "def standardise_date(date):\n",
    "    date = str(date).strip()\n",
    "    if re.match(r\"^\\d{4}-\\d{2}-\\d{2}$\", date):\n",
    "        return date  # yyyy-mm-dd -> yyyy-mm-dd\n",
    "    elif re.match(r\"^\\d{2}/\\d{2}/\\d{4}$\", date):\n",
    "        match = re.match(r\"^(\\d{2})/(\\d{2})/(\\d{4})$\", date)\n",
    "        return f\"{match.group(3)}-{match.group(1)}-{match.group(2)}\"  # mm/dd/yyyy -> yyyy-mm-dd\n",
    "    elif re.match(r\"^(\\d{2})/(\\d{2})$\", date):\n",
    "        match = re.match(r\"^(\\d{2})/(\\d{2})$\", date)\n",
    "        return f\"20{match.group(2)}-{match.group(1)}-01\"  # mm/dd -> yyyy-mm-dd\n",
    "    elif re.match(r\"^\\d{2}-\\d{2}-\\d{4}$\", date):\n",
    "        match = re.match(r\"^(\\d{2})-(\\d{2})-(\\d{4})$\", date)\n",
    "        return f\"{match.group(3)}-{match.group(2)}-{match.group(1)}\"  # dd-mm-yyyy -> yyyy-mm-dd\n",
    "    elif re.match(r\"^\\d{2}-\\d{2}-\\d{2}$\", date):\n",
    "        match = re.match(r\"^(\\d{2})-(\\d{2})-(\\d{2})$\", date)\n",
    "        return f\"20{match.group(3)}-{match.group(2)}-{match.group(1)}\"  # dd-mm-yy -> yyyy-mm-dd\n",
    "    return date\n",
    "\n",
    "for col in standardize_date_columns:\n",
    "    dataset[col] = dataset[col].apply(standardise_date) #to standardize the values in each column of the dataset"
   ],
   "id": "2d0ae65127993e39",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Handling outliers (Outliers detection)\n",
    "\n",
    "# Quartiles and IQR for outliers detection\n",
    "Q1 = dataset['PanierMoyen'].quantile(0.25)\n",
    "Q3 = dataset['PanierMoyen'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# bounds definition for outliers detection\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# outliers identification\n",
    "outliers = dataset[(dataset['PanierMoyen'] < lower_bound) | (dataset['PanierMoyen'] > upper_bound)]\n",
    "print(f\"Outliers numbers find : {len(outliers)}\")\n",
    "display(outliers)\n",
    "\n",
    "# PanierMoyen graphic representation\n",
    "\n",
    "# Boxplot of PanierMoyen\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.boxplot(x=dataset['PanierMoyen'])\n",
    "plt.title(\"Boxplot of PanierMoyen\")\n",
    "plt.show()\n",
    "\n",
    "# Scatterplot of NombreAchats and PanierMoyen\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.scatterplot(x=dataset['NombreAchats'], y=dataset['PanierMoyen'])\n",
    "plt.title(\"Scatterplot of NombreAchats and PanierMoyen\")\n",
    "plt.xlabel(\"Nombre d'Achats\")\n",
    "plt.ylabel(\"Panier Moyen\")\n",
    "plt.show()"
   ],
   "id": "f35003b548ad3f8f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Standardizing data types\n",
    "\n",
    "# data types detection\n",
    "print(f\"The dataset data types before cleaning are : \\n {dataset.dtypes}\") #to see the data types of the dataset\n",
    "\n",
    "# Standardizing the data types of the columns\n",
    "dataset['DateNaissance'] = pd.to_datetime(dataset['DateNaissance']) #to convert the data type of the column to datetime\n",
    "dataset['DernierAchat'] = pd.to_datetime(dataset['DernierAchat']) #to convert the data type of the column to datetime\n",
    "\n",
    "print(f\"The dataset data types after cleaning are : \\n {dataset.dtypes}\") #to see the data types of the dataset"
   ],
   "id": "39e5f6631c00c023",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3. Data Transformation (Anonymization, pseudonomization, columns selection etc...)\n",
    "Data transformation is the process of converting raw data into a format that is suitable for analysis. This step involves several key tasks, including:\n",
    "- [X] Anonymization\n",
    "- [X] Pseudonymization\n",
    "- [X] Aggregation\n",
    "- [X] Data reduction\n",
    "- [X] Data Addition"
   ],
   "id": "ce79e579e4a2500d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Anonymization\n",
    "\n",
    "# Nom et PreÃÅnom : Anonymisation\n",
    "def anonymize_name(name):\n",
    "    # return hashlib.sha256(name.encode()).hexdigest()[:8]\n",
    "    return hashlib.md5(name.encode()).hexdigest()\n",
    "\n",
    "dataset['Nom'] = dataset['Nom'].astype(str).apply(anonymize_name)\n",
    "dataset['Pr√©nom'] = dataset['Pr√©nom'].astype(str).apply(anonymize_name)\n",
    "\n",
    "# Adresse email : Anonymisation\n",
    "def anonymize_email(email):\n",
    "    username = email.split('@')[0]\n",
    "    hashed_username = hashlib.md5(username.encode()).hexdigest()\n",
    "    return f\"{hashed_username}@masked.com\"\n",
    "\n",
    "dataset['Email'] = dataset['Email'].astype(str).apply(anonymize_email)\n",
    "\n",
    "# Adresse : Anonymisation\n",
    "dataset['Adresse'] = \"masked\"\n",
    "\n",
    "# TeÃÅleÃÅphone : Anonymisation\n",
    "dataset['T√©l√©phone'] = \"masked\""
   ],
   "id": "f45d8161d8edf1a7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Pseudonymization\n",
    "\n",
    "# Pseudonymization of the NumeroCarteCredit column\n",
    "def pseudonymize_credit_card(card_number):\n",
    "    return f\"**** **** **** {card_number[-4:]}\"  # to keep only the last 4 digits of the credit card number\n",
    "\n",
    "dataset['Num√©roCarteCr√©dit'] = dataset['Num√©roCarteCr√©dit'].astype(str).apply(pseudonymize_credit_card)\n",
    "\n",
    "# Pseudonymization of the DateExpirationCarte column\n",
    "def pseudonymize_credit_card_expiration(expiration_date):\n",
    "    return expiration_date[-2:]  # to keep only the last 2 digits of the expiration date\n",
    "\n",
    "dataset['DateExpirationCarte'] = dataset['DateExpirationCarte'].astype(str).apply(pseudonymize_credit_card_expiration)\n",
    "\n",
    "# Pseudonymization of the code postal column\n",
    "def pseudonymize_postal_code(postal_code):\n",
    "    return postal_code[:2] + \"***\"  # to keep only the first 2 digits of the postal code"
   ],
   "id": "c0c8d4b4cfd7b6e5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Aggregation\n",
    "\n",
    "# Aggregation of the age column\n",
    "bins = [0, 18, 25, 35, 45, 55, 65, 100]  # age gap definition\n",
    "labels = [\"0-18\", \"19-25\", \"26-35\", \"36-45\", \"46-55\", \"56-65\", \"65+\"]\n",
    "\n",
    "dataset['√Çge'] = pd.cut(dataset['√Çge'], bins=bins, labels=labels)"
   ],
   "id": "9d28d0f6fa9f0421",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Data Reduction\n",
    "\n",
    "# Columns_to_delete = ['CodePostal', 'Nom', 'Pr√©nom', 'Email', 'Adresse', 'T√©l√©phone', 'DateNaissance', 'Num√©roCarteCr√©dit', 'DateExpirationCarte', 'SoldeCompte']\n",
    "# dataset.drop(columns=Columns_to_delete, inplace=True) #to delete the columns that we don't need anymore"
   ],
   "id": "658085976fc03822",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Data Addition\n",
    "# Addition of columns\n",
    "\n"
   ],
   "id": "93aa27f40321f17c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 4. Validation\n",
    "Data validation is the process of ensuring that the data is accurate, complete, and consistent. This step involves several key tasks, including:\n",
    "- [X] Data profiling"
   ],
   "id": "cd16d3b57713f8e6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Data profiling after cleaning and transformation\n",
    "\n",
    "# Display the first few rows of the dataset after cleaning and transformation\n",
    "\n",
    "print(f\"The clean dataset head is : \\n {dataset.head()}\") #to see a quick view of the dataset"
   ],
   "id": "fbc121e30a2defb3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Display the infos of the dataset\n",
    "\n",
    "print(f\"The dataset info is : \\n {dataset.info()}\") #to see the structure of the dataset"
   ],
   "id": "7589772ebfb38d5e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Display the shape of the dataset\n",
    "\n",
    "print(f\"This dataset have {dataset.shape[0]} entries and {dataset.shape[1]} columns after the cleaning and the transformation.\")"
   ],
   "id": "7e0e6c32eb8abaa1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Description of the dataset after cleaning and transformation\n",
    "print(f\"The dataset describe is : \\n {dataset.describe()}\") #to see the statistical summary of the dataset"
   ],
   "id": "d74498c26d9783a7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Data types verification\n",
    "print(f\"The dataset data types are : \\n {dataset.dtypes}\") #to see the data types of the dataset"
   ],
   "id": "8ecb3d92cf0349c6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# missing values verification\n",
    "print(dataset.isnull().sum()) #to see the number of missing values in each column of the dataset"
   ],
   "id": "a023d9eabf2667e5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 5. Data Export (To CSV)\n",
    "The final step in the data processing pipeline is to export the cleaned and transformed data to a file format that can be used for analysis. This step involves exporting the data to a CSV or Excel file, which can then be imported into a data visualization tool for further analysis."
   ],
   "id": "6f1e70ade75cca53"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Exporting the cleaned and transformed data to a CSV file\n",
    "export_path = \"data/processed/Mini_Projet_Evaluation_Cleaned_transform.csv\"\n",
    "\n",
    "dataset.to_csv(export_path, index=False, encoding='utf-8')\n",
    "print(f\"The file was exported successfully : {export_path}\")"
   ],
   "id": "eb6212e08230c2dc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 6. Data Visualization (BONUS)\n",
    "\n",
    "Data visualization is the process of representing data graphically to help data engineers and analysts understand the data and identify patterns and trends. Data visualization is a critical step in the data processing pipeline, as it helps to communicate the results of the analysis to stakeholders and decision-makers. Data visualization involves several key tasks, including:"
   ],
   "id": "a4cd9097658b2132"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "4545d3c19f1e3a86",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
